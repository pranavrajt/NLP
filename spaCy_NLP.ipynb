{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier,export_graphviz\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import tree\n",
    "from IPython.display import SVG\n",
    "from IPython.display import display\n",
    "from subprocess import call\n",
    "import pydotplus\n",
    "from sklearn.externals.six import StringIO\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"London is the capital and most populous city of England and \n",
    "the United Kingdom.  Standing on the River Thames in the south east \n",
    "of the island of Great Britain, London has been a major settlement \n",
    "for two millennia. It was founded by the Romans, who named it Londinium.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London is the capital and most populous city of England and \n",
      "the United Kingdom.  Standing on the River Thames in the south east \n",
      "of the island of Great Britain, London has been a major settlement \n",
      "for two millennia. It was founded by the Romans, who named it Londinium.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the text in the document\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.remove_pipe('sentencizer')\n",
    "nlp.add_pipe(sbd,first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London is the capital and most populous city of England and \n",
      "the United Kingdom.\n",
      " Standing on the River Thames in the south east \n",
      "of the island of Great Britain, London has been a major settlement \n",
      "for two millennia.\n",
      "It was founded by the Romans, who named it Londinium.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sentence based tokenizer\n",
    "sbd = nlp.create_pipe('sentencizer')\n",
    "doc1 = nlp(text)\n",
    "\n",
    "sents_list = []\n",
    "for sen in doc1.sents:\n",
    "    print(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[London, capital, populous, city, England, \n",
      ", United, Kingdom, .,  , Standing, River, Thames, south, east, \n",
      ", island, Great, Britain, ,, London, major, settlement, \n",
      ", millennia, ., founded, Romans, ,, named, Londinium, ., \n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#remove stop words using spaCy\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "filtered_sen =[]\n",
    "for words in doc:\n",
    "    if words.is_stop == False: # is_stop is a command to check if the word is a stop word or not\n",
    "        filtered_sen.append(words)\n",
    "print(filtered_sen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London London\n",
      "is be\n",
      "the the\n",
      "capital capital\n",
      "and and\n",
      "most most\n",
      "populous populous\n",
      "city city\n",
      "of of\n",
      "England England\n",
      "and and\n",
      "\n",
      " \n",
      "\n",
      "the the\n",
      "United United\n",
      "Kingdom Kingdom\n",
      ". .\n",
      "   \n",
      "Standing stand\n",
      "on on\n",
      "the the\n",
      "River River\n",
      "Thames Thames\n",
      "in in\n",
      "the the\n",
      "south south\n",
      "east east\n",
      "\n",
      " \n",
      "\n",
      "of of\n",
      "the the\n",
      "island island\n",
      "of of\n",
      "Great Great\n",
      "Britain Britain\n",
      ", ,\n",
      "London London\n",
      "has have\n",
      "been be\n",
      "a a\n",
      "major major\n",
      "settlement settlement\n",
      "\n",
      " \n",
      "\n",
      "for for\n",
      "two two\n",
      "millennia millennium\n",
      ". .\n",
      "It -PRON-\n",
      "was be\n",
      "founded found\n",
      "by by\n",
      "the the\n",
      "Romans Romans\n",
      ", ,\n",
      "who who\n",
      "named name\n",
      "it -PRON-\n",
      "Londinium Londinium\n",
      ". .\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization of words\n",
    "for words in doc:\n",
    "    print(words.text,words.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    London\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " is the capital and most populous city of \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    England\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and </br>\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    the United Kingdom\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".  Standing on \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    the River Thames\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " in the south east </br>of the island of \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Great Britain\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    London\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " has been a major settlement </br>for \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    two millennia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ". It was founded by the \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Romans\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       ", who named it \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Londinium\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ".\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visual Rendering entity objects\n",
    "from spacy import displacy\n",
    "displacy.render(doc,style='ent',jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London is the capital and most populous city of England\n",
      "and \n",
      "the United Kingdom.  Standing on the\n"
     ]
    }
   ],
   "source": [
    "# slicing of documents\n",
    "first_sentence = doc[0:10]\n",
    "second_sentence = doc[10:20]\n",
    "print(first_sentence.text)\n",
    "print(second_sentence.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last word found:  Kingdom\n",
      "Last word found:  Britain\n",
      "Last word found:  millennia\n",
      "Last word found:  Romans\n",
      "Last word found:  Londinium\n"
     ]
    }
   ],
   "source": [
    "#lexical attributes, lets find the words ocurring before punctuations. (this idea can be transferred for number mainly)\n",
    "for token in doc:\n",
    "    if token.is_alpha:\n",
    "        temp_word = doc[token.i+1] # the i+1 helps pick the next word. we can edit to suit our needs\n",
    "        if temp_word.is_punct:\n",
    "            print(\"Last word found: \", token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London GPE\n",
      "England GPE\n",
      "the United Kingdom GPE\n",
      "the River Thames LOC\n",
      "Great Britain GPE\n",
      "London GPE\n",
      "two millennia DATE\n",
      "Romans NORP\n",
      "Londinium LOC\n"
     ]
    }
   ],
   "source": [
    "#Named entities are \"real world objects\" that are assigned a name – for example, a person, an organization or a country.\n",
    "\n",
    "#The doc dot ents property lets you access the named entities predicted by the model.\n",
    "\n",
    "#It returns an iterator of Span objects, so we can print the entity text and the entity label using \n",
    "#the \"label underscore\" attribute.\n",
    "\n",
    "#In this case, the model is correctly predicting \"London\" as an Geopolitical Entity, \"U.K.\" as a geopolitical entity \n",
    "#and \"two millenia\" as date and so on and so forth.\n",
    "\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London PROPN\n",
      "is VERB\n",
      "the DET\n",
      "capital NOUN\n",
      "and CCONJ\n",
      "most ADV\n",
      "populous ADJ\n",
      "city NOUN\n",
      "of ADP\n",
      "England PROPN\n",
      "and CCONJ\n",
      "\n",
      " SPACE\n",
      "the DET\n",
      "United PROPN\n",
      "Kingdom PROPN\n",
      ". PUNCT\n",
      "  SPACE\n",
      "Standing VERB\n",
      "on ADP\n",
      "the DET\n",
      "River PROPN\n",
      "Thames PROPN\n",
      "in ADP\n",
      "the DET\n",
      "south ADJ\n",
      "east NOUN\n",
      "\n",
      " SPACE\n",
      "of ADP\n",
      "the DET\n",
      "island NOUN\n",
      "of ADP\n",
      "Great PROPN\n",
      "Britain PROPN\n",
      ", PUNCT\n",
      "London PROPN\n",
      "has VERB\n",
      "been VERB\n",
      "a DET\n",
      "major ADJ\n",
      "settlement NOUN\n",
      "\n",
      " SPACE\n",
      "for ADP\n",
      "two NUM\n",
      "millennia NOUN\n",
      ". PUNCT\n",
      "It PRON\n",
      "was VERB\n",
      "founded VERB\n",
      "by ADP\n",
      "the DET\n",
      "Romans PROPN\n",
      ", PUNCT\n",
      "who PRON\n",
      "named VERB\n",
      "it PRON\n",
      "Londinium PROPN\n",
      ". PUNCT\n",
      "\n",
      " SPACE\n"
     ]
    }
   ],
   "source": [
    "# analogous to above example, use spacy to identify part of speech tags\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London PROPN nsubj is\n",
      "is VERB ROOT is\n",
      "the DET det capital\n",
      "capital NOUN attr is\n",
      "and CCONJ cc capital\n",
      "most ADV advmod populous\n",
      "populous ADJ amod city\n",
      "city NOUN conj capital\n",
      "of ADP prep city\n",
      "England PROPN pobj of\n",
      "and CCONJ cc city\n",
      "\n",
      " SPACE  and\n",
      "the DET det Kingdom\n",
      "United PROPN compound Kingdom\n",
      "Kingdom PROPN conj city\n",
      ". PUNCT punct is\n",
      "  SPACE  .\n",
      "Standing VERB advcl been\n",
      "on ADP prep Standing\n",
      "the DET det Thames\n",
      "River PROPN compound Thames\n",
      "Thames PROPN pobj on\n",
      "in ADP prep Standing\n",
      "the DET det east\n",
      "south ADJ amod east\n",
      "east NOUN pobj in\n",
      "\n",
      " SPACE  east\n",
      "of ADP prep east\n",
      "the DET det island\n",
      "island NOUN pobj of\n",
      "of ADP prep island\n",
      "Great PROPN compound Britain\n",
      "Britain PROPN pobj of\n",
      ", PUNCT punct been\n",
      "London PROPN nsubj been\n",
      "has VERB aux been\n",
      "been VERB ROOT been\n",
      "a DET det settlement\n",
      "major ADJ amod settlement\n",
      "settlement NOUN attr been\n",
      "\n",
      " SPACE  settlement\n",
      "for ADP prep settlement\n",
      "two NUM nummod millennia\n",
      "millennia NOUN pobj for\n",
      ". PUNCT punct been\n",
      "It PRON nsubjpass founded\n",
      "was VERB auxpass founded\n",
      "founded VERB ROOT founded\n",
      "by ADP agent founded\n",
      "the DET det Romans\n",
      "Romans PROPN pobj by\n",
      ", PUNCT punct Romans\n",
      "who PRON nsubj named\n",
      "named VERB relcl Romans\n",
      "it PRON dobj named\n",
      "Londinium PROPN oprd named\n",
      ". PUNCT punct founded\n",
      "\n",
      " SPACE  .\n"
     ]
    }
   ],
   "source": [
    "#In addition to the part-of-speech tags, we can also predict how the words are related. \n",
    "#For example, whether a word is the subject of the sentence or an object.\n",
    "\n",
    "#The \"dep underscore\" attribute returns the predicted dependency label.\n",
    "\n",
    "#The head attribute returns the syntactic head token. You can also think of it as the parent token \n",
    "#this word is attached to.\n",
    "\n",
    "for token in doc:\n",
    "    print (token.text, token.pos_, token.dep_, token.head.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"06fdd5cc75614a4992a5930f72784d61-0\" class=\"displacy\" width=\"2675\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">am</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">rather</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">disappointed</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">by</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">Laurent</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">Koscielny</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">'s</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">decision</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">leave</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">Arsenal</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">this</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">fashion</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-06fdd5cc75614a4992a5930f72784d61-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-06fdd5cc75614a4992a5930f72784d61-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-06fdd5cc75614a4992a5930f72784d61-0-1\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-06fdd5cc75614a4992a5930f72784d61-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-06fdd5cc75614a4992a5930f72784d61-0-2\" stroke-width=\"2px\" d=\"M245,264.5 C245,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-06fdd5cc75614a4992a5930f72784d61-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,266.5 L578.0,254.5 562.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-06fdd5cc75614a4992a5930f72784d61-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-06fdd5cc75614a4992a5930f72784d61-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">agent</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M740.0,266.5 L748.0,254.5 732.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-06fdd5cc75614a4992a5930f72784d61-0-4\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-06fdd5cc75614a4992a5930f72784d61-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-06fdd5cc75614a4992a5930f72784d61-0-5\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,89.5 1445.0,89.5 1445.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-06fdd5cc75614a4992a5930f72784d61-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-06fdd5cc75614a4992a5930f72784d61-0-6\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-06fdd5cc75614a4992a5930f72784d61-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1265.0,266.5 L1273.0,254.5 1257.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-06fdd5cc75614a4992a5930f72784d61-0-7\" stroke-width=\"2px\" d=\"M770,264.5 C770,2.0 1450.0,2.0 1450.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-06fdd5cc75614a4992a5930f72784d61-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1450.0,266.5 L1458.0,254.5 1442.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-06fdd5cc75614a4992a5930f72784d61-0-8\" stroke-width=\"2px\" d=\"M1645,264.5 C1645,177.0 1790.0,177.0 1790.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-06fdd5cc75614a4992a5930f72784d61-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1645,266.5 L1637,254.5 1653,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-06fdd5cc75614a4992a5930f72784d61-0-9\" stroke-width=\"2px\" d=\"M1470,264.5 C1470,89.5 1795.0,89.5 1795.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-06fdd5cc75614a4992a5930f72784d61-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1795.0,266.5 L1803.0,254.5 1787.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-06fdd5cc75614a4992a5930f72784d61-0-10\" stroke-width=\"2px\" d=\"M1820,264.5 C1820,177.0 1965.0,177.0 1965.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-06fdd5cc75614a4992a5930f72784d61-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1965.0,266.5 L1973.0,254.5 1957.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-06fdd5cc75614a4992a5930f72784d61-0-11\" stroke-width=\"2px\" d=\"M1820,264.5 C1820,89.5 2145.0,89.5 2145.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-06fdd5cc75614a4992a5930f72784d61-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2145.0,266.5 L2153.0,254.5 2137.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-06fdd5cc75614a4992a5930f72784d61-0-12\" stroke-width=\"2px\" d=\"M2345,264.5 C2345,177.0 2490.0,177.0 2490.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-06fdd5cc75614a4992a5930f72784d61-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2345,266.5 L2337,254.5 2353,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-06fdd5cc75614a4992a5930f72784d61-0-13\" stroke-width=\"2px\" d=\"M2170,264.5 C2170,89.5 2495.0,89.5 2495.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-06fdd5cc75614a4992a5930f72784d61-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2495.0,266.5 L2503.0,254.5 2487.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visual Rendering of dependency parsing\n",
    "text1 = \"I am rather disappointed by Laurent Koscielny's decision to leave Arsenal in this fashion\"\n",
    "doc2 = nlp(text1)\n",
    "displacy.render(doc2,style='dep',jupyter=\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object of preposition\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Countries, cities, states'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A quick tip: To get definitions for the most common tags and labels, \n",
    "#you can use the spacy dot explain helper function.\n",
    "#use it POS, NER and anything\n",
    "print(spacy.explain('pobj'))\n",
    "spacy.explain('GPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Rule Based Matching\n",
    "Compared to regular expressions, the matcher works with Doc and Token objects instead of only strings.\n",
    "\n",
    "It's also more flexible: you can search for texts but also other lexical attributes.\n",
    "\n",
    "You can even write rules that use the model's predictions.\n",
    "\n",
    "For example, find the word \"duck\" only if it's a verb, not a noun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match patterns are lists of dictionaries. Each dictionary describes one token. The keys are the names of token attributes, mapped to their expected values.\n",
    "\n",
    "In this example, we're looking for two tokens with the text \"iPhone\" and \"X\".\n",
    "\n",
    "We can also match on other token attributes. Here, we're looking for two tokens whose lowercase forms equal \"iphone\" and \"x\".\n",
    "\n",
    "We can even write patterns using attributes predicted by the model. Here, we're matching a token with the lemma \"buy\", plus a noun. The lemma is the base form, so this pattern would match phrases like \"buying milk\" or \"bought flowers\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match exact token texts\n",
    "\n",
    "[{'TEXT': 'iPhone'}, {'TEXT': 'X'}]\n",
    "Match lexical attributes\n",
    "[{'LOWER': 'iphone'}, {'LOWER': 'x'}]\n",
    "Match any token attributes\n",
    "[{'LEMMA': 'buy'}, {'POS': 'NOUN'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a pattern, we first import the matcher from spacy dot matcher.\n",
    "\n",
    "We also load a model and create the nlp object.\n",
    "\n",
    "The matcher is initialized with the shared vocabulary, nlp dot vocab. You'll learn more about this later – for now, just remember to always pass it in.\n",
    "\n",
    "The matcher dot add method lets you add a pattern. The first argument is a unique ID to identify which pattern was matched. The second argument is an optional callback. We don't need one here, so we set it to None. The third argument is the pattern.\n",
    "\n",
    "To match the pattern on a text, we can call the matcher on any doc.\n",
    "\n",
    "This will return the matches."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "EXAMPLE CODE-\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load a model and create the nlp object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "pattern = [{'TEXT': 'iPhone'}, {'TEXT': 'X'}]\n",
    "matcher.add('IPHONE_PATTERN', None, pattern)\n",
    "\n",
    "# Process some text\n",
    "doc = nlp(\"New iPhone X release date leaked\")\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you call the matcher on a doc, it returns a list of tuples.\n",
    "\n",
    "Each tuple consists of three values: the match ID, the start index and the end index of the matched span.\n",
    "\n",
    "This means we can iterate over the matches and create a Span object: a slice of the doc at the start and end index.m"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Call the matcher on the doc\n",
    "\n",
    "doc = nlp(\"New iPhone X release date leaked\")\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "#Iterate over the matches\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "\n",
    "    #Get the matched span\n",
    "    \n",
    "    matched_span = doc[start:end]\n",
    "    \n",
    "    print(matched_span.text)\n",
    "    \n",
    "iPhone X\n",
    "\n",
    "match_id: hash value of the pattern name\n",
    "\n",
    "start: start index of matched span\n",
    "\n",
    "end: end index of matched span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of a more complex pattern using lexical attributes.\n",
    "\n",
    "We're looking for five tokens:\n",
    "\n",
    "A token consisting of only digits.\n",
    "\n",
    "Three case-insensitive tokens for \"fifa\", \"world\" and \"cup\".\n",
    "\n",
    "And a token that consists of punctuation.\n",
    "\n",
    "The pattern matches the tokens \"2018 FIFA World Cup:\"."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pattern = [\n",
    "    {'IS_DIGIT': True},\n",
    "    {'LOWER': 'fifa'},\n",
    "    {'LOWER': 'world'},\n",
    "    {'LOWER': 'cup'},\n",
    "    {'IS_PUNCT': True}\n",
    "]\n",
    "\n",
    "doc = nlp(\"2018 FIFA World Cup: France won!\")\n",
    "\n",
    "OUTPUT\n",
    "2018 FIFA World Cup:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we're looking for two tokens:\n",
    "\n",
    "A verb with the lemma \"love\", followed by a noun.\n",
    "\n",
    "This pattern will match \"loved dogs\" and \"love cats\"."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pattern = [\n",
    "    {'LEMMA': 'love', 'POS': 'VERB'},\n",
    "    {'POS': 'NOUN'}\n",
    "]\n",
    "\n",
    "doc = nlp(\"I loved dogs but now I love cats more.\")\n",
    "\n",
    "OUTPUT\n",
    "\n",
    "loved dogs\n",
    "love cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operators and quantifiers let you define how often a token should be matched. They can be added using the \"OP\" key.\n",
    "\n",
    "Here, the \"?\" operator makes the determiner token optional, so it will match a token with the lemma \"buy\", an optional article and a noun."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pattern = [\n",
    "    {'LEMMA': 'buy'},\n",
    "    {'POS': 'DET', 'OP': '?'},  # optional: match 0 or 1 times\n",
    "    {'POS': 'NOUN'}\n",
    "]\n",
    "\n",
    "doc = nlp(\"I bought a smartphone. Now I'm buying apps.\")\n",
    "\n",
    "OUTPUT\n",
    "\n",
    "bought a smartphone\n",
    "buying apps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"OP\" can have one of four values:\n",
    "\n",
    "An \"!\" negates the token, so it's matched 0 times.\n",
    "\n",
    "A \"?\" makes the token optional, and matches it 0 or 1 times.\n",
    "\n",
    "A \"+\" matches a token 1 or more times.\n",
    "\n",
    "And finally, an \"*\" matches 0 or more times.\n",
    "\n",
    "Operators can make your patterns a lot more powerful, but they also add more complexity – so use them wisely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example\tDescription\n",
    "\n",
    "{'OP': '!'}\tNegation: match 0 times\n",
    "    \n",
    "{'OP': '?'}\tOptional: match 0 or 1 times\n",
    "    \n",
    "{'OP': '+'}\tMatch 1 or more times\n",
    "\n",
    "{'OP': '*'}\tMatch 0 or more times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#practice rule based matching\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standing on\n",
      "founded by\n"
     ]
    }
   ],
   "source": [
    "pattern = [{'POS':'VERB'},{'POS':'ADP'}]\n",
    "matcher.add('UNITED_KINGDOM_MATCHER',None,pattern)\n",
    "matches = matcher(doc)\n",
    "for match_id,start,end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the hash for a string, we can look it up in nlp dot vocab dot strings.\n",
    "\n",
    "To get the string representation of a hash, we can look up the hash.\n",
    "\n",
    "A Doc object also exposes its vocab and strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5392354317538386956\n"
     ]
    }
   ],
   "source": [
    "print(nlp.vocab.strings['London'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London\n"
     ]
    }
   ],
   "source": [
    "print(nlp.vocab.strings[5392354317538386956])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13226800834791099135\n"
     ]
    }
   ],
   "source": [
    "print(doc.vocab.strings['United'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check OneNote for infograph on Lexemes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example. Let's say we want to find out whether two documents are similar.\n",
    "\n",
    "First, we load the medium English model, \"en_core_web_md\".\n",
    "\n",
    "We can then create two doc objects and use the first doc's similarity method to compare it to the second.\n",
    "\n",
    "Here, a fairly high similarity score of 0.86 is predicted for \"I like fast food\" and \"I like pizza\".\n",
    "\n",
    "The same works for tokens.\n",
    "\n",
    "According to the word vectors, the tokens \"pizza\" and \"pasta\" are kind of similar, and receive a score of 0.7.`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Load a larger model with vectors\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "#Compare two documents\n",
    "\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "\n",
    "print(doc1.similarity(doc2))\n",
    "\n",
    "0.8627204117787385\n",
    "\n",
    "#Compare two tokens\n",
    "\n",
    "doc = nlp(\"I like pizza and pasta\")\n",
    "\n",
    "token1 = doc[2]\n",
    "\n",
    "token2 = doc[4]\n",
    "\n",
    "print(token1.similarity(token2))\n",
    "\n",
    "0.7369546"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the similarity methods to compare different types of objects.\n",
    "\n",
    "For example, a document and a token.\n",
    "\n",
    "Here, the similarity score is pretty low and the two objects are considered fairly dissimilar.\n",
    "\n",
    "Here's another example comparing a span – \"pizza and pasta\" – to a document about McDonalds.\n",
    "\n",
    "The score returned here is 0.61, so it's determined to be kind of similar."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Compare a document with a token\n",
    "\n",
    "doc = nlp(\"I like pizza\")\n",
    "\n",
    "token = nlp(\"soap\")[0]\n",
    "\n",
    "print(doc.similarity(token))\n",
    "\n",
    "0.32531983166759537\n",
    "\n",
    "#Compare a span with a document\n",
    "\n",
    "span = nlp(\"I like pizza and pasta\")[2:5]\n",
    "\n",
    "doc = nlp(\"McDonalds sells burgers\")\n",
    "\n",
    "print(span.similarity(doc))\n",
    "\n",
    "0.619909235817623"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how does spaCy do this under the hood?\n",
    "\n",
    "Similarity is determined using word vectors, multi-dimensional representations of meanings of words.\n",
    "\n",
    "You might have heard of Word2Vec, which is an algorithm that's often used to train word vectors from raw text.\n",
    "\n",
    "Vectors can be added to spaCy's statistical models.\n",
    "\n",
    "By default, the similarity returned by spaCy is the cosine similarity between two vectors – but this can be adjusted if necessary.\n",
    "\n",
    "Vectors for objects consisting of several tokens, like the Doc and Span, default to the average of their token vectors.\n",
    "\n",
    "That's also why you usually get more value out of shorter phrases with fewer irrelevant words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting similarity can be useful for many types of applications. For example, to recommend a user similar texts based on the ones they have read. It can also be helpful to flag duplicate content, like posts on an online platform.\n",
    "\n",
    "However, it's important to keep in mind that there's no objective definition of what's similar and what isn't. It always depends on the context and what your application needs to do.\n",
    "\n",
    "Here's an example: spaCy's default word vectors assign a very high similarity score to \"I like cats\" and \"I hate cats\". This makes sense, because both texts express sentiment about cats. But in a different application context, you might want to consider the phrases as very dissimilar, because they talk about opposite sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give you an idea of what those vectors look like, here's an example.\n",
    "\n",
    "First, we load the medium model again, which ships with word vectors.\n",
    "\n",
    "Next, we can process a text and look up a token's vector using the dot vector attribute.\n",
    "\n",
    "The result is a 300-dimensional vector of the word \"banana\"."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Load a larger model with vectors\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "doc = nlp(\"I have a banana\")\n",
    "\n",
    "#Access the vector via the token.vector attribute\n",
    "\n",
    "print(doc[3].vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuitive explanation for word vector\n",
    "Word Vector Representation\n",
    "When we’re looking at words alone, it’s difficult for a machine to understand connections that a human would understand immediately. Engine and car, for example, have what might seem like an obvious connection (cars run using engines), but that link is not so obvious to a computer.\n",
    "\n",
    "Thankfully, there’s a way we can represent words that captures more of these sorts of connections. A word vector is a numeric representation of a word that commuicates its relationship to other words.\n",
    "\n",
    "Each word is interpreted as a unique and lenghty array of numbers. You can think of these numbers as being something like GPS coordinates. GPS coordinates consist of two numbers (latitude and longitude), and if we saw two sets GPS coordinates that were numberically close to each other (like 43,-70, and 44,-70), we would know that those two locations were relatively close together. Word vectors work similarly, although there are a lot more than two coordinates assigned to each word, so they’re much harder for a human to eyeball.\n",
    "\n",
    "Using spaCy‘s en_core_web_sm model, let’s take a look at the length of a vector for a single word, and what that vector looks like using .vector and .shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.4961e-02  5.0200e-01  2.3823e-03 -1.6755e-01  3.0721e-01 -2.3762e-01\n",
      "  1.6069e-01 -3.6786e-01 -5.8347e-02  2.4990e+00 -2.3647e-03  1.0732e-02\n",
      " -3.0422e-01  8.4579e-02 -4.0299e-02 -4.1562e-01 -2.4494e-02  1.4691e+00\n",
      " -5.2932e-02 -7.4413e-02 -3.9244e-01 -3.2535e-01 -2.2333e-01  5.6823e-03\n",
      "  3.5675e-01  1.9445e-01  5.6762e-02 -4.5502e-02 -2.8105e-01 -5.8896e-02\n",
      " -9.8626e-02  9.2177e-02  3.3172e-01 -3.9967e-02 -1.1766e-01  4.8373e-02\n",
      " -6.2241e-02 -1.0413e-01  9.9263e-04 -4.8925e-01  3.4786e-01  3.2724e-01\n",
      "  1.3882e-01 -1.9917e-01  1.2995e-01  6.0549e-02 -2.3714e-01 -5.1295e-01\n",
      " -3.7396e-01  1.2902e-01  5.5797e-02  3.3444e-01 -1.8025e-01 -3.4740e-02\n",
      "  2.8323e-01 -9.5301e-02  2.1143e-01 -7.6149e-02  1.5069e-01 -1.7441e-01\n",
      " -7.4768e-03 -7.8287e-02 -1.2751e-01  2.2545e-01  3.5101e-02 -6.1015e-01\n",
      " -2.6812e-01  6.1632e-02 -3.0503e-01 -1.3405e-01 -4.4271e-01 -1.7720e-01\n",
      "  1.7663e-01 -3.1210e-01 -2.5722e-01 -2.4858e-02  7.2504e-02 -7.9759e-02\n",
      " -1.9214e-01  5.9602e-01  1.2880e-01 -7.4629e-02 -1.5812e-01  3.6394e-01\n",
      "  2.3055e-01 -4.2175e-01 -9.0651e-02 -3.0085e-01  1.7940e-01 -2.9786e-01\n",
      " -1.0642e-01  4.7239e-01 -1.3837e-01 -1.0161e-01  8.0134e-02  4.0715e-02\n",
      " -3.6976e-01 -3.7066e-02  1.0436e-01  1.7904e-01  1.5702e-01 -7.4670e-02\n",
      " -2.9431e-01  1.2829e-01  5.5211e-02 -4.3906e-01 -6.8231e-02  9.7107e-02\n",
      " -2.8209e-01 -8.6528e-02 -2.4204e-01  2.8734e-02 -2.1509e-01  2.7152e-02\n",
      " -1.7996e-01  1.9317e-01 -2.7929e-01  2.9415e-01 -1.0965e-01 -1.0432e-01\n",
      " -5.2170e-01 -4.6789e-02  1.3743e-01 -1.5518e-01 -1.0359e-01  1.8853e-01\n",
      " -1.2684e-01 -6.7278e-01  3.4483e-02 -2.2937e-01 -9.8073e-02 -7.0157e-02\n",
      "  8.4374e-02  2.6594e-01  2.3104e-01 -2.9251e-01 -8.7209e-02 -2.3342e-01\n",
      "  6.3759e-02 -1.3556e-01 -8.4046e-01  2.4681e-01  3.0498e-01  3.5438e-01\n",
      "  1.4137e-01 -3.6720e-01  2.3321e-01 -1.5497e-01  4.8364e-01  1.4711e-02\n",
      " -2.4176e-01  3.7589e-02  1.9829e-01 -6.9403e-02 -1.7362e-03  4.1694e-02\n",
      " -3.4193e-01 -2.0034e-01 -4.5581e-01 -1.2504e-01  1.3954e-01  3.2275e-02\n",
      " -5.2130e-03  4.5422e-02 -2.6574e-03 -2.6266e-01  6.4168e-02 -1.4231e-01\n",
      "  3.5709e-04 -2.3253e-01  2.7615e-02 -7.4282e-02  1.8671e-01 -1.2994e-01\n",
      " -4.3731e-01  1.4550e-01  4.4838e-02 -1.9022e-01 -1.5401e-01  1.4188e-01\n",
      "  9.8269e-02 -4.2930e-02 -2.7478e-01 -3.3224e-01 -3.2167e-01 -1.0509e-01\n",
      " -1.9816e-01 -6.5097e-02 -9.1251e-02  1.9528e-01 -3.3297e-01 -1.5504e-01\n",
      " -4.7688e-01  3.1985e-01  1.9886e-01  1.1501e-01  5.5757e-02 -5.4307e-02\n",
      "  2.8851e-01  2.7982e-01  1.3960e-02 -1.2891e-03 -2.3128e-01  7.5396e-02\n",
      "  4.3587e-02 -1.3937e-01 -6.2935e-02  1.2568e-01  9.5235e-02 -8.5203e-02\n",
      " -2.4241e-01 -4.8771e-02  9.5937e-02 -2.2347e-01  2.3503e-01  3.1517e-01\n",
      " -1.4900e-02  2.1739e-01 -1.9431e-01 -2.3255e-01 -2.2961e-01  4.8297e-02\n",
      "  1.6050e-01 -1.6100e-02  4.2770e-02 -3.2367e-01  1.4680e-01  2.4551e-01\n",
      "  7.5506e-02 -3.9703e-02 -1.0321e-01  1.6194e-01  2.7132e-01  4.6348e-02\n",
      " -9.2743e-02 -1.4929e-01  2.7378e-01 -3.6958e-01 -4.1530e-01  1.8402e-01\n",
      " -4.9775e-02  6.9670e-02  1.3447e-01  1.7788e-01 -4.7586e-02 -3.6491e-01\n",
      " -1.3733e-01 -4.8119e-01  2.4681e-01 -8.9842e-02  3.7939e-02 -1.8284e-01\n",
      "  4.7012e-01 -9.9584e-02 -1.8365e-01 -7.1821e-02  4.1607e-01 -1.8581e-01\n",
      "  1.8400e-01 -2.9028e-02  4.1228e-01  2.2856e-02  5.0915e-02 -1.1911e-01\n",
      "  8.1231e-02  1.3845e-01  4.6595e-02 -4.3974e-02  6.3601e-01  3.7101e-03\n",
      "  9.3937e-02 -9.3442e-02 -4.7606e-01 -2.6427e-01 -2.3044e-02 -5.8241e-02\n",
      "  1.1440e-01 -5.1702e-02  3.5225e-01  2.5341e-01  5.7256e-01  2.2867e-01\n",
      "  8.5401e-03 -6.2531e-02 -3.2118e-02 -1.5647e-01 -8.4344e-02  7.6667e-02\n",
      "  3.4515e-01 -1.9452e-01  8.7003e-02 -7.8201e-02 -6.9673e-02 -1.6993e-01\n",
      "  2.3598e-01  2.7550e-01 -6.7180e-02 -2.1511e-01 -2.6304e-01 -6.0173e-03]\n"
     ]
    }
   ],
   "source": [
    "print(doc[1].vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"London is the capital and most populous city of England and \n",
    "the United Kingdom. Arsenal is based out of this great city.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9648870102443627\n"
     ]
    }
   ],
   "source": [
    "print(doc1.similarity(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical models are useful if your application needs to be able to generalize based on a few examples.\n",
    "\n",
    "For instance, detecting product or person names usually benefits from a statistical model. Instead of providing a list of all person names ever, your application will be able to predict whether a span of tokens is a person name. Similarly, you can predict dependency labels to find subject/object relationships.\n",
    "\n",
    "To do this, you would use spaCy's entity recognizer, dependency parser or part-of-speech tagger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule-based approaches on the other hand come in handy if there's a more or less finite number of instances you want to find. For example, all countries or cities of the world, drug names or even dog breeds.\n",
    "\n",
    "In spaCy, you can achieve this with custom tokenization rules, as well as the matcher and phrase matcher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of a matcher rule for \"golden retriever\".\n",
    "\n",
    "If we iterate over the matches returned by the matcher, we can get the match ID and the start and end index of the matched span. We can then find out more about it. Span objects give us access to the original document and all other token attributes and linguistic features predicted by the model.\n",
    "\n",
    "For example, we can get the span's root token. If the span consists of more than one token, this will be the token that decides the category of the phrase. For example, the root of \"Golden Retriever\" is \"Retriever\". We can also find the head token of the root. This is the syntactic \"parent\" that governs the phrase – in this case, the verb \"have\".\n",
    "\n",
    "Finally, we can look at the previous token and its attributes. In this case, it's a determiner, the article \"a\"."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('DOG', None, [{'LOWER': 'golden'}, {'LOWER': 'retriever'}])\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print('Matched span:', span.text)\n",
    "    # Get the span's root token and root head token\n",
    "    print('Root token:', span.root.text)\n",
    "    print('Root head token:', span.root.head.text)\n",
    "    # Get the previous token and its POS tag\n",
    "    print('Previous token:', doc[start - 1].text, doc[start - 1].pos_)\n",
    "    \n",
    "RESULT\n",
    "Matched span: Golden Retriever\n",
    "Root token: Retriever\n",
    "Root head token: have\n",
    "Previous token: a DET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The phrase matcher is another helpful tool to find sequences of words in your data.\n",
    "\n",
    "It performs a keyword search on the document, but instead of only finding strings, it gives you direct access to the tokens in context.\n",
    "\n",
    "It takes Doc objects as patterns.\n",
    "\n",
    "It's also really fast.\n",
    "\n",
    "This makes it very useful for matching large dictionaries and word lists on large volumes of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example.\n",
    "\n",
    "The phrase matcher can be imported from spacy dot matcher and follows the same API as the regular matcher.\n",
    "\n",
    "Instead of a list of dictionaries, we pass in a Doc object as the pattern.\n",
    "\n",
    "We can then iterate over the matches in the text, which gives us the match ID, and the start and end of the match. This lets us create a Span object for the matched tokens \"Golden Retriever\" to analyze it in context."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp(\"Golden Retriever\")\n",
    "matcher.add('DOG', None, pattern)\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Get the matched span\n",
    "    span = doc[start:end]\n",
    "    print('Matched span:', span.text)\n",
    "Matched span: Golden Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "doc = nlp(\n",
    "    \"Twitch Prime, the perks program for Amazon Prime members offering free \"\n",
    "    \"loot, games and other benefits, is ditching one of its best features: \"\n",
    "    \"ad-free viewing. According to an email sent out to Amazon Prime members \"\n",
    "    \"today, ad-free viewing will no longer be included as a part of Twitch \"\n",
    "    \"Prime for new members, beginning on September 14. However, members with \"\n",
    "    \"existing annual subscriptions will be able to continue to enjoy ad-free \"\n",
    "    \"viewing until their subscription comes up for renewal. Those with \"\n",
    "    \"monthly subscriptions will have access to ad-free viewing until October 15.\"\n",
    ")\n",
    "\n",
    "# Create the match patterns\n",
    "pattern1 = [{\"TEXT\": \"Amazon\"}, {\"IS_TITLE\": True, \"POS\": \"PROPN\"}]\n",
    "pattern2 = [{\"TEXT\": \"ad\"},{\"PUNCT\":'-'},{\"TEXT\":\"free\"}, {\"POS\": \"NOUN\"}]\n",
    "\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PATTERN1\", None, pattern1)\n",
    "matcher.add(\"PATTERN2\", None, pattern2)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Amazon', 'Prime']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.text for token in nlp(\"Amazon Prime\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it’s more efficient to match exact strings instead of writing patterns describing the individual tokens. This is especially true for finite categories of things – like all countries of the world. We already have a list of countries, so let’s use this as the basis of our information extraction script. A list of string names is available as the variable COUNTRIES."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Below example and see a new style of describing patterns different from the Golden retriever example"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import json\n",
    "from spacy.lang.en import English\n",
    "\n",
    "with open(\"exercises/countries.json\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "nlp = English()\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "\n",
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(COUNTRIES)) #--- this part\n",
    "matcher.add(\"COUNTRY\", None, *patterns) #---this part\n",
    "\n",
    "# Call the matcher on the test document and print the result\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've already written this plenty of times by now: pass a string of text to the nlp object, and receive a Doc object.\n",
    "\n",
    "But what does the nlp object actually do?\n",
    "\n",
    "First, the tokenizer is applied to turn the string of text into a Doc object. Next, a series of pipeline components is applied to the Doc in order. In this case, the tagger, then the parser, then the entity recognizer. Finally, the processed Doc is returned, so you can work with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy ships with the following built-in pipeline components.\n",
    "\n",
    "The part-of-speech tagger sets the token dot tag attribute.\n",
    "\n",
    "The dependency parser adds the token dot dep and token dot head attributes and is also responsible for detecting sentences and base noun phrases, also known as noun chunks.\n",
    "\n",
    "The named entity recognizer adds the detected entities to the doc dot ents property. It also sets entity type attributes on the tokens that indicate if a token is part of an entity or not.\n",
    "\n",
    "Finally, the text classifier sets category labels that apply to the whole text, and adds them to the doc dot cats property.\n",
    "\n",
    "Because text categories are always very specific, the text classifier is not included in any of the pre-trained models by default. But you can use it to train your own system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the names of the pipeline components present in the current nlp object, you can use the nlp dot pipe names attribute.\n",
    "\n",
    "For a list of component name and component function tuples, you can use the nlp dot pipeline attribute.\n",
    "\n",
    "The component functions are the functions applied to the Doc to process it and set attributes – for example, part-of-speech tags or named entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know how spaCy's pipeline works, let's take a look at another very powerful feature: custom pipeline components.\n",
    "\n",
    "Custom pipeline components let you add your own function to the spaCy pipeline that is executed when you call the nlp object on a text – for example, to modify the Doc and add more data to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the text is tokenized and a Doc object has been created, pipeline components are applied in order. spaCy supports a range of built-in components, but also lets you define your own.\n",
    "\n",
    "Custom components are executed automatically when you call the nlp object on a text.\n",
    "\n",
    "They're especially useful for adding your own custom metadata to documents and tokens.\n",
    "\n",
    "You can also use them to update built-in attributes, like the named entity spans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fundamentally, a pipeline component is a function or callable that takes a doc, modifies it and returns it, so it can be processed by the next component in the pipeline.\n",
    "\n",
    "Components can be added to the pipeline using the nlp dot add pipe method. The method takes at least one argument: the component function."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def custom_component(doc):\n",
    "    # Do something to the doc here\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(custom_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To specify where to add the component in the pipeline, you can use the following keyword arguments:\n",
    "\n",
    "Setting last to True will add the component last in the pipeline. This is the default behavior.\n",
    "\n",
    "Setting first to True will add the component first in the pipeline, right after the tokenizer.\n",
    "\n",
    "The \"before\" and \"after\" arguments let you define the name of an existing component to add the new component before or after. For example, before equals \"ner\" will add it before the named entity recognizer.\n",
    "\n",
    "The other component to add the new component before or after needs to exist, though – otherwise, spaCy will raise an error."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "last\tIf True, add last\tnlp.add_pipe(component, last=True)\n",
    "first\tIf True, add first\tnlp.add_pipe(component, first=True)\n",
    "before\tAdd before component\tnlp.add_pipe(component, before='ner')\n",
    "after\tAdd after component\tnlp.add_pipe(component, after='tagger')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of a simple pipeline component.\n",
    "\n",
    "We start off with the small English model.\n",
    "\n",
    "We then define the component – a function that takes a Doc object and returns it.\n",
    "\n",
    "Let's do something simple and print the length of the Doc that passes through the pipeline.\n",
    "\n",
    "Don't forget to return the Doc so it can be processed by the next component in the pipeline! The Doc created by the tokenizer is passed through all components, so it's important that they all return the modified doc.\n",
    "\n",
    "We can now add the component to the pipeline. Let's add it to the very beginning right after the tokenizer by setting first equals True.\n",
    "\n",
    "When we print the pipeline component names, the custom component now shows up at the start. This means it will be applied first when we process a Doc."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create the nlp object\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define a custom component\n",
    "def custom_component(doc):\n",
    "    # Print the doc's length\n",
    "    print('Doc length:', len(doc))\n",
    "    # Return the doc object\n",
    "    return doc\n",
    "\n",
    "# Add the component first in the pipeline\n",
    "nlp.add_pipe(custom_component, first=True)\n",
    "\n",
    "# Print the pipeline component names\n",
    "print('Pipeline:', nlp.pipe_names)\n",
    "Pipeline: ['custom_component', 'tagger', 'parser', 'ner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upper(doc1):\n",
    "    upper_doc = len(doc1)\n",
    "    print(upper_doc)\n",
    "    return doc1\n",
    "\n",
    "#nlp.add_pipe(upper,first=True)\n",
    "doc1 = nlp('text')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
